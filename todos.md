## Objective

The goal of this project is to apply everything we have learned
in this course to build an end-to-end RAG application.

## Problem statement

For the project, we ask you to build an end-to-end RAG project.

For that, you need:

- [x] Select a dataset that you're interested in (see [Datasets](#datasets) for examples and ideas)
- [ ] Ingest the data into a knowledge base
- [ ] Implement the retrieval flow: query the knowledge base, build the prompt, send the promt to an LLM
- [ ] Evaluate the performance of your RAG flow
- [ ] Create an interface for the application
- [ ] Collect user feedback and monitor your application

## Project Documentation

Your project rises or falls with its documentation. Hence, here are some general recommendations:

- **Write for a Broader Audience ğŸ“**: Assume the reader has no prior knowledge of the course materials. This way, your documentation will be accessible not only to evaluators but also to anyone interested in your project.
- **Include Evaluation Criteria ğŸ¯**: Make it easier for evaluators to assess your work by clearly mentioning each criterion in your README. Include relevant screenshots to visually support your points.
- **Think of Future Opportunities ğŸš€**: Imagine that potential hiring managers will look at your projects. Make it straightforward for them to understand what the project is about and what you contributed. Highlight key features and your role in the project.
- **Be Detailed and Comprehensive ğŸ“‹**: Include as much detail as possible in the README file. Explain the setup, the functionality, and the workflow of your project. Tools like ChatGPT or other LLMs can assist you in expanding and refining your documentation.
- **Provide Clear Setup Instructions âš™ï¸**: Include step-by-step instructions on how to set up and run your project locally. Make sure to cover dependencies, configurations, and any other requirements needed to get your project up and running.
- **Use Visuals and Examples ğŸ–¼ï¸**: Wherever possible, include diagrams, screenshots, or GIFs to illustrate key points. Use examples to show how to use your project, demonstrate common use cases, and provide sample inputs and expected outputs.
  - **App Preview Video ğŸ¥**: Consider adding a short preview video of your app in action to the README. For example, if you're using Streamlit, you can easily record a screencast from the app's top-right menu ([Streamlit Guide](https://docs.streamlit.io/develop/concepts/architecture/app-chrome)). Once you saved the video file locally, you can just drag & drop it into the online GitHub editor of your README to add it ([Ref](https://stackoverflow.com/a/4279746)).
- **Organize with Sub-Files ğŸ—‚ï¸**: If your documentation becomes lengthy, consider splitting it into sub-files and linking them in your README. This keeps the main README clean and neat while providing additional detailed information in separate files (e.g., `setup.md`, `usage.md`, `contributing.md`).
- **Keep It Updated ğŸ”„**: As your project evolves, make sure your documentation reflects any changes or updates. Outdated documentation can confuse readers and diminish the credibility of your project.

Remember, clear and comprehensive documentation not only helps others but is also a valuable reference for yourself in the future.

## Technologies

You don't have to limit yourself to technologies covered in the course. You can use alternatives as well:

- LLM: OpenAI, **Ollama**, Groq, AWS Bedrock, etc
- Knowledge base: any text, relational or vector database, including in-memory ones like we implemented in the course or SQLite
- Monitoring: Grafana, Kibana, Streamlit, dash, etc
- Interface: Streamlit, dash, Flask, FastAPI, Django, etc (could be UI or API)
- Ingestion pipeline: Mage, dlt, Airflow, Prefect, python script, etc

If you use a tool that wasn't covered in the course, be sure to give a very detailed explanation
of what that tool does and how to use it.

If you're not certain about some tools, ask in Slack.

## Tips and best practices

- It's better to create a separate GitHub repository for your project
- Give your project a meaningful title, e.g. "DataTalksClub Zoomcamp Q&A system" or "Nutrition Facts Chat"

## Evaluation Criteria

- Problem description
  - 0 points: The problem is not described
  - 1 point: The problem is described but briefly or unclearly
  - 2 points: The problem is well-described and it's clear what problem the project solves
- Retrieval flow
  - 0 points: No knowledge base or LLM is used
  - 1 point: No knowledge base is used, and the LLM is queried directly
  - 2 points: Both a knowledge base and an LLM are used in the flow
- Retrieval evaluation
  - 0 points: No evaluation of retrieval is provided
  - 1 point: Only one retrieval approach is evaluated
  - 2 points: Multiple retrieval approaches are evaluated, and the best one is used
- LLM evaluation
  - 0 points: No evaluation of final LLM output is provided
  - 1 point: Only one approach (e.g., one prompt) is evaluated
  - 2 points: Multiple approaches are evaluated, and the best one is used
- Interface
  - 0 points: No way to interact with the application at all
  - 1 point: Command line interface, a script, or a Jupyter notebook
  - 2 points: UI (e.g., Streamlit), web application (e.g., Django), or an API (e.g., built with FastAPI)
- Ingestion pipeline
  - 0 points: No ingestion
  - 1 point: Semi-automated ingestion of the dataset into the knowledge base, e.g., with a Jupyter notebook
  - 2 points: Automated ingestion with a Python script or a special tool (e.g., Mage, dlt, Airflow, Prefect)
- Monitoring
  - 0 points: No monitoring
  - 1 point: User feedback is collected OR there's a monitoring dashboard
  - 2 points: User feedback is collected and there's a dashboard with at least 5 charts
- Containerization
  - 0 points: No containerization
  - 1 point: Dockerfile is provided for the main application OR there's a docker-compose for the dependencies only
  - 2 points: Everything is in docker-compose
- Reproducibility
  - 0 points: No instructions on how to run the code, the data is missing, or it's unclear how to access it
  - 1 point: Some instructions are provided but are incomplete, OR instructions are clear and complete, the code works, but the data is missing
  - 2 points: Instructions are clear, the dataset is accessible, it's easy to run the code, and it works. The versions for all dependencies are specified.
- Best practices
  - [ ] Hybrid search: combining both text and vector search (at least evaluating it) (1 point)
  - [ ] Document re-ranking (1 point)
  - [ ] User query rewriting (1 point)
- Bonus points (not covered in the course)
  - [ ] Deployment to the cloud (2 points)
  - [ ] Up to 3 extra bonus points if you want to award for something extra (write in feedback for what)

## TODOs

- data ingestion
  - [ ] injest blog contestns itself
  - [ ] injest posts based on search result of algolia
- knowledge base
  - [ ] vector search
  - [ ] Hybrid search
- discovery
  - [ ] knowledge graph
- ops
  - [ ] docker compose
  - [ ] deploy on hamravesh
  - [ ] k8s manifest
  - [ ] helm chart
- monitoring
- UI
  - [ ] API for chat
  - [ ] compatible with ollama api interface
- evaluation
  - [ ] **ground truth data**
- monitoring
- RAG
  - [ ] simple RAG
  - [ ] agentic RAG
- tools
  - [ ] chat & evaluate
  - [ ] with Response Schema
  - [ ] context decorator
- project structure and best practices

```
temporallm/
â”œâ”€â”€ cmd/                          # Application entry points
â”‚   â”œâ”€â”€ client/                   # Temporal client application
â”‚   â”‚   â””â”€â”€ main.go
â”‚   â”œâ”€â”€ worker/                   # Temporal worker application
â”‚   â”‚   â””â”€â”€ main.go
â”‚   â””â”€â”€ api/                      # HTTP API server (if needed)
â”‚       â””â”€â”€ main.go
â”œâ”€â”€ internal/                     # Private application code
â”‚   â”œâ”€â”€ config/                   # Configuration management
â”‚   â”‚   â”œâ”€â”€ config.go
â”‚   â”‚   â””â”€â”€ env.go
â”‚   â”œâ”€â”€ domain/                   # Business logic and domain models
â”‚   â”‚   â”œâ”€â”€ models/               # Data structures and types
â”‚   â”‚   â”‚   â”œâ”€â”€ document.go
â”‚   â”‚   â”‚   â”œâ”€â”€ hacker_news.go
â”‚   â”‚   â”‚   â””â”€â”€ message.go
â”‚   â”‚   â”œâ”€â”€ services/             # Business logic services
â”‚   â”‚   â”‚   â”œâ”€â”€ chatbot_service.go
â”‚   â”‚   â”‚   â”œâ”€â”€ search_service.go
â”‚   â”‚   â”‚   â””â”€â”€ indexing_service.go
â”‚   â”‚   â””â”€â”€ repositories/         # Data access layer
â”‚   â”‚       â”œâ”€â”€ elasticsearch_repo.go
â”‚   â”‚       â””â”€â”€ hacker_news_repo.go
â”‚   â”œâ”€â”€ infrastructure/           # External service integrations
â”‚   â”‚   â”œâ”€â”€ elasticsearch/        # Elasticsearch client and operations
â”‚   â”‚   â”‚   â”œâ”€â”€ client.go
â”‚   â”‚   â”‚   â”œâ”€â”€ index.go
â”‚   â”‚   â”‚   â””â”€â”€ search.go
â”‚   â”‚   â”œâ”€â”€ llm/                  # LLM integration
â”‚   â”‚   â”‚   â”œâ”€â”€ client.go
â”‚   â”‚   â”‚   â”œâ”€â”€ models.go
â”‚   â”‚   â”‚   â””â”€â”€ prompts.go
â”‚   â”‚   â””â”€â”€ temporal/             # Temporal workflow orchestration
â”‚   â”‚       â”œâ”€â”€ workflows.go
â”‚   â”‚       â”œâ”€â”€ activities.go
â”‚   â”‚       â””â”€â”€ client.go
â”‚   â”œâ”€â”€ api/                      # HTTP API layer (if needed)
â”‚   â”‚   â”œâ”€â”€ handlers/             # HTTP request handlers
â”‚   â”‚   â”œâ”€â”€ middleware/           # HTTP middleware
â”‚   â”‚   â””â”€â”€ routes/               # Route definitions
â”‚   â””â”€â”€ utils/                    # Shared utilities
â”‚       â”œâ”€â”€ logger/               # Logging utilities
â”‚       â”œâ”€â”€ errors/               # Error handling
â”‚       â””â”€â”€ validation/           # Input validation
â”œâ”€â”€ pkg/                          # Public libraries (if any)
â”‚   â””â”€â”€ public/                   # Reusable packages
â”œâ”€â”€ scripts/                      # Build and deployment scripts
â”‚   â”œâ”€â”€ build.sh
â”‚   â”œâ”€â”€ deploy.sh
â”‚   â””â”€â”€ migrate.sh
â”œâ”€â”€ deployments/                  # Deployment configurations
â”‚   â”œâ”€â”€ docker/                   # Docker configurations
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ kubernetes/               # K8s manifests
â”‚   â””â”€â”€ terraform/                # Infrastructure as code
â”œâ”€â”€ docs/                         # Documentation
â”‚   â”œâ”€â”€ api/                      # API documentation
â”‚   â”œâ”€â”€ architecture/             # Architecture diagrams
â”‚   â””â”€â”€ guides/                   # User guides
â”œâ”€â”€ test/                         # Integration tests
â”‚   â”œâ”€â”€ fixtures/                 # Test data
â”‚   â””â”€â”€ integration/              # Integration test suites
â”œâ”€â”€ .env.example                  # Environment variables template
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ go.mod
â”œâ”€â”€ go.sum
â”œâ”€â”€ Makefile                      # Build automation
â”œâ”€â”€ README.md
â””â”€â”€ CHANGELOG.md
```

Key Architectural Principles

1. Domain-Driven Design (DDD)

```
internal/domain/
â”œâ”€â”€ models/          # Core business entities
â”œâ”€â”€ services/        # Business logic
â””â”€â”€ repositories/    # Data access abstraction
```

2. Clean Architecture Layers
   Domain Layer: Pure business logic
   Infrastructure Layer: External integrations
   Application Layer: Use cases and workflows
   Interface Layer: API handlers and CLI commands
3. Dependency Injection

```go
// Example structure
type ChatbotService struct {
    llmClient    LLMClient
    searchRepo   SearchRepository
    hnRepo       HackerNewsRepository
}
```

4. Configuration Management

```go
// internal/config/config.go
type Config struct {
    Temporal   TemporalConfig
    Elasticsearch ElasticsearchConfig
    LLM        LLMConfig
    Server     ServerConfig
}
```

5. Error Handling Strategy

```go
// internal/utils/errors/errors.go
type AppError struct {
    Code    string
    Message string
    Err     error
}
```

Specific Recommendations for Your Project

1. Reorganize Current Structure
   Move internal/chatbot/ logic to internal/domain/services/
   Move internal/elasticsearch/ to internal/infrastructure/elasticsearch/
   Move internal/llm/ to internal/infrastructure/llm/
   Move internal/temporal/ to internal/infrastructure/temporal/
2. Add Missing Components
   Repository Pattern: Abstract data access behind interfaces
   Service Layer: Centralize business logic
   Error Handling: Consistent error types and handling
   Logging: Structured logging with levels
   Testing: Unit and integration test structure
3. Configuration Improvements
   Environment-specific configs (dev, staging, prod)
   Validation of required environment variables
   Secrets management integration
4. API Layer (Optional)
   RESTful API endpoints for your workflows
   OpenAPI/Swagger documentation
   Rate limiting and authentication
5. Monitoring and Observability
   Health check endpoints
   Metrics collection (Prometheus)
   Distributed tracing
   Structured logging
   This architecture provides:
   Scalability: Easy to add new features and services
   Maintainability: Clear separation of concerns
   Testability: Dependency injection and interfaces
   Deployability: Clear deployment configurations
   Observability: Built-in monitoring and logging
